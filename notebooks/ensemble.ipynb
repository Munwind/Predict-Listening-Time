{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"},{"sourceId":7074842,"sourceType":"datasetVersion","datasetId":4074593},{"sourceId":7570020,"sourceType":"datasetVersion","datasetId":4021289},{"sourceId":9335009,"sourceType":"datasetVersion","datasetId":5656615},{"sourceId":11592717,"sourceType":"datasetVersion","datasetId":7269510},{"sourceId":11608476,"sourceType":"datasetVersion","datasetId":7281204},{"sourceId":11616691,"sourceType":"datasetVersion","datasetId":7285105},{"sourceId":11617014,"sourceType":"datasetVersion","datasetId":7287270},{"sourceId":203746103,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Neural Network","metadata":{}},{"cell_type":"code","source":"%%time\n!pip install -q --no-index -U --find-links=/kaggle/input/tensorflow-2-15/tensorflow tensorflow==2.15.0\n!pip install -q --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5\n!pip install -q --no-index -U --find-links=/kaggle/input/fix-deeptables/deeptables-0.2.6 deeptables==0.2.6","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-29T10:06:43.534834Z","iopub.execute_input":"2025-04-29T10:06:43.536010Z","iopub.status.idle":"2025-04-29T10:06:58.238290Z","shell.execute_reply.started":"2025-04-29T10:06:43.535965Z","shell.execute_reply":"2025-04-29T10:06:58.236887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport shap\nimport math\nimport ctypes\nimport random\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd\nfrom colorama import Fore, Style\nfrom itertools import combinations\nfrom numpy.typing import ArrayLike\nfrom joblib import Parallel, delayed\nfrom sklearn.base import BaseEstimator\nfrom sklearn.model_selection import KFold\nfrom category_encoders import TargetEncoder\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport tensorflow as tf, deeptables as dt\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom deeptables.utils.shap import DeepTablesExplainer\nfrom deeptables.models import DeepTable, ModelConfig, deepnets\n\nwarnings.filterwarnings('ignore')\nprint('TensorFlow version:',tf.__version__+',',\n      'GPU =',tf.test.is_gpu_available())\nprint('DeepTables version:',dt.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:06:58.240532Z","iopub.execute_input":"2025-04-29T10:06:58.240929Z","iopub.status.idle":"2025-04-29T10:06:58.254610Z","shell.execute_reply.started":"2025-04-29T10:06:58.240885Z","shell.execute_reply":"2025-04-29T10:06:58.252585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(seed=42)\n\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\nclean_memory()\n\ndef print_memory_usage(X, X_test, wording='default'):\n    print(f\"Memory usage {wording}      X: {X.memory_usage(deep=True).sum() / (1024*1024):.2f} MB\")\n    print(f\"Memory usage {wording} X_test: {X_test.memory_usage(deep=True).sum() / (1024*1024):.2f} MB\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:06:58.257526Z","iopub.execute_input":"2025-04-29T10:06:58.257927Z","iopub.status.idle":"2025-04-29T10:06:59.315185Z","shell.execute_reply.started":"2025-04-29T10:06:58.257881Z","shell.execute_reply":"2025-04-29T10:06:59.313983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s5e4/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e4/test.csv\")\ntrain_orig = pd.read_csv('/kaggle/input/podcast-listening-time-prediction-dataset/podcast_dataset.csv')\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)\nprint(\"Train original shape:\", train_orig.shape, '\\n')\n\ntrain = pd.concat([train, train_orig], ignore_index=True).drop_duplicates()\ntrain.dropna(subset=['Listening_Time_minutes'], inplace=True)\ntrain.reset_index(drop=True, inplace=True)\nprint(\"Train combied shape:\", train.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:06:59.318116Z","iopub.execute_input":"2025-04-29T10:06:59.318462Z","iopub.status.idle":"2025-04-29T10:07:01.004243Z","shell.execute_reply.started":"2025-04-29T10:06:59.318434Z","shell.execute_reply":"2025-04-29T10:07:01.002717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ELM = []\nfor k in range(3):\n    col_name = f'ELm_r{k}'\n    train[col_name] = train['Episode_Length_minutes'].round(k)\n    test[col_name] = test['Episode_Length_minutes'].round(k)\n    ELM.append(col_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:07:01.005561Z","iopub.execute_input":"2025-04-29T10:07:01.006316Z","iopub.status.idle":"2025-04-29T10:07:01.020592Z","shell.execute_reply.started":"2025-04-29T10:07:01.006267Z","shell.execute_reply":"2025-04-29T10:07:01.019277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def target_encoding(train, target, test=None, feat_to_encode=None, min_samples_leaf=1, smoothing=0.1):\n    train.sort_index(inplace=True)\n    if feat_to_encode is None:\n        feat_to_encode = train.columns.tolist()\n    encoder_params = dict(cols=feat_to_encode, min_samples_leaf=min_samples_leaf, smoothing=smoothing)\n    \n    oof_parts = []\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    for tr_idx, val_idx in kf.split(train, target):\n        encoder = TargetEncoder(**encoder_params)\n        encoder.fit(train.iloc[tr_idx], target.iloc[tr_idx])\n        encoded = encoder.transform(train.iloc[val_idx])\n        encoded[feat_to_encode] = encoded[feat_to_encode].astype('float32')\n        encoded.index = train.index[val_idx]\n        oof_parts.append(encoded)\n        \n    final_encoder = encoder = TargetEncoder(**encoder_params)\n    final_encoder.fit(train, target)\n    if test is not None:\n        test = final_encoder.transform(test)\n        test[feat_to_encode] = test[feat_to_encode].astype('float32')\n        \n    train = pd.concat(oof_parts).sort_index()\n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:07:01.021960Z","iopub.execute_input":"2025-04-29T10:07:01.022404Z","iopub.status.idle":"2025-04-29T10:07:01.047442Z","shell.execute_reply.started":"2025-04-29T10:07:01.022361Z","shell.execute_reply":"2025-04-29T10:07:01.046010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nX = train.drop(['id', 'Listening_Time_minutes'], axis=1)\ny = train.Listening_Time_minutes\nX_test = test.drop(['id'], axis=1)\ndel train, test\nprint(\"X      shape:\", X.shape)\nprint(\"X_test shape:\", X_test.shape, '\\n')\n\ncat_cols = X.select_dtypes(include=['object']).columns.tolist()\nnum_cols = X.select_dtypes(exclude=['object']).columns.tolist()\nprint(\"init len(cat_cols):\", len(cat_cols))\nprint(\"init len(num_cols):\", len(num_cols), '\\n')\n\n# Only 1 missing value to fill\nm_ = X['Number_of_Ads'].mode()[0] \nX['Number_of_Ads'] = X['Number_of_Ads'].fillna(m_)\n# Fill missing values and create an indicator column\nfor c in num_cols:\n    if X[c].isna().any():\n        m = X[c].mean()\n        X[f'NA_{c}'] = X[c].isna().astype('int8')\n        X[c] = X[c].fillna(m)\n        X_test[f'NA_{c}'] = X_test[c].isna().astype('int8')\n        X_test[c] = X_test[c].fillna(m)\n        num_cols.append(f'NA_{c}')\n\n\npair_size = [2, 3, 4]\nencode_cols = ['Episode_Length_minutes',\n               'Number_of_Ads',\n#               'Episode_Title',\n               'Episode_Sentiment',\n               'Publication_Day',\n               'Publication_Time',\n               'Podcast_Name',\n#               'Genre',\n               'Guest_Popularity_percentage',\n               'Host_Popularity_percentage']\n\ndef eng_combos(df):\n    df_str_np = df[encode_cols].astype(str).values.astype('U')\n    encoded_columns = []\n    selected_comb = [\n        ['Episode_Num', 'Host_Popularity_percentage'],\n        ['Episode_Num', 'Guest_Popularity_percentage'],\n        ['Episode_Num', 'Number_of_Ads'],    \n        ['ELm_r1', 'Episode_Num'],\n        ['ELm_r1', 'Host_Popularity_percentage'], \n        ['ELm_r1', 'Guest_Popularity_percentage'],\n        ['ELm_r2', 'Episode_Num'],\n        ['ELm_r2', 'Episode_Sentiment'],\n        ['ELm_r2', 'Publication_Day'],\n        ['ELm_r1', 'Number_of_Ads', 'Episode_Sentiment'],\n        ['ELm_r2', 'Number_of_Ads', 'Podcast_Name'],\n        ['Episode_Num', 'Podcast_Name'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Episode_Sentiment'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Day'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Time'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Genre'],\n        ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n        ['Episode_Num', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n        ['Episode_Num', 'Guest_Popularity_percentage', 'Publication_Day'],\n        ['Episode_Num', 'Guest_Popularity_percentage', 'Publication_Time'],\n        ['Episode_Num', 'Guest_Popularity_percentage', 'Genre'],\n        ['Episode_Num', 'Number_of_Ads', 'Episode_Sentiment'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Publication_Day'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Publication_Time'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Genre'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Publication_Day'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Publication_Time'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Episode_Sentiment'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Publication_Day'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Publication_Time'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day', 'Publication_Time'],\n        ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day', 'Genre'],    \n        ['Episode_Length_minutes', 'Episode_Num', 'Publication_Time', 'Podcast_Name'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Time'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Publication_Day'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Publication_Time'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Genre'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time'],\n        ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Time', 'Genre'],\n        ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n        ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],\n    ]\n\n    for comb in selected_comb:\n        name = '.' + '_'.join(comb)\n            \n        if len(comb) == 2:\n            df[name] = df[comb[0]].astype(str) + '_' + df[comb[1]].astype(str)\n            \n        elif len(comb) == 3:\n            df[name] = (df[comb[0]].astype(str) + '_' +\n                           df[comb[1]].astype(str) + '_' +\n                           df[comb[2]].astype(str))\n            \n        elif len(comb) == 4:\n            df[name] = (df[comb[0]].astype(str) + '_' +\n                           df[comb[1]].astype(str) + '_' +\n                           df[comb[2]].astype(str) + '_' +\n                           df[comb[3]].astype(str))\n    \n        encoded_columns.append(name)\n\n    df[encoded_columns] = df[encoded_columns].astype('category')\n    for r in pair_size:\n        for cols in combinations(range(len(encode_cols)), r):\n            col_names = [encode_cols[i] for i in cols]\n            new_col_name = '._' + '_'.join(col_names)\n            concat = df_str_np[:, cols[0]]\n            for i in range(1, r):\n                concat = np.char.add(np.char.add(concat, '_'), df_str_np[:, cols[i]])\n            df[new_col_name] = pd.Categorical(concat)\n    \n    return df\n\n\n\ndef feat_eng(df, num_chunks=4, n_jobs=4):\n    df['_Has_Ads'] = (df['Number_of_Ads'] > 0).astype('int8')\n    df['_Is_Weekend'] = df['Publication_Day'].isin(['Saturday', 'Sunday']).astype('int8')\n    df['_sqrt_Episode_Length_minutes'] = np.sqrt(df['Episode_Length_minutes']).astype('float32')\n    df['_squared_Episode_Length_minutes'] = (df['Episode_Length_minutes'] ** 2).astype('float32')\n    df['_sin_Episode_Length_minutes'] = np.sin(2*np.pi * df['Episode_Length_minutes'] / 60).astype('float32')\n    df['_cos_Episode_Length_minutes'] = np.cos(2*np.pi * df['Episode_Length_minutes'] / 60).astype('float32')\n    df['_sin_Host_Popularity_percentage'] = np.sin(2*np.pi * df['Host_Popularity_percentage'] / 20).astype('float32')\n    df['_cos_Host_Popularity_percentage'] = np.cos(2*np.pi * df['Host_Popularity_percentage'] / 20).astype('float32')\n    df['Episode_Num'] = df['Episode_Title'].str[8:]     \n    df['is_weekend']   = df['Publication_Day'].isin(['Saturday', 'Sunday']).astype(int)\n    \n    time_dict = {'Morning': 0, 'Afternoon': 1, 'Evening': 2, 'Night': 3}\n    df['Publication_Time_enc'] = df['Publication_Time'].replace(time_dict)\n    df['_sin_Publication_Time'] = np.sin(2*np.pi * df['Publication_Time_enc'] / 2).astype('float32')\n    df['_cos_Publication_Time'] = np.cos(2*np.pi * df['Publication_Time_enc'] / 2).astype('float32')\n    df = df.drop(['Publication_Time_enc'], axis=1)\n    \n    chunks = np.array_split(df, num_chunks)\n    results = Parallel(n_jobs=n_jobs)(delayed(eng_combos)(chunk) for chunk in chunks)\n    df = pd.concat(results, ignore_index=True)\n            \n    new_cat_cols = [col for col in df.columns if col.endswith('_')]\n    new_num_cols = [col for col in df.columns if col.startswith('_')]\n    new_enc_cols = [col for col in df.columns if col.startswith('.')]\n    return df, new_cat_cols, new_num_cols, new_enc_cols\n    \nX, new_cat_cols, new_num_cols, new_enc_cols = feat_eng(X)\nX_test, new_cat_cols, new_num_cols, new_enc_cols = feat_eng(X_test)\nnum_cols += new_num_cols; cat_cols += new_cat_cols\nprint(\"len(new_cat_cols):\", len(new_cat_cols))\nprint(\"len(new_num_cols):\", len(new_num_cols)+2)  # +2 NA indicator columns\nprint(\"len(new_enc_cols):\", len(new_enc_cols), '\\n')\nprint_memory_usage(X, X_test, wording='after feat eng')\nclean_memory()\n\n# Reduce memory usage\nX_all = pd.concat([X, X_test])\nfor col in X_all.columns:\n    if col.startswith('._'):\n        X_all[col] = X_all[col].astype('category').cat.codes.astype('int32')\nX = X_all.iloc[:len(X)]; X_test = X_all.iloc[len(X):]\nprint_memory_usage(X, X_test, wording='after reduce')\ndel X_all; clean_memory()\n\nX, X_test = target_encoding(X, y, X_test, feat_to_encode=new_enc_cols)\nprint_memory_usage(X, X_test, wording='after encode')\nclean_memory()\n\nscaler = QuantileTransformer(subsample=10**9)\nX[num_cols] = scaler.fit_transform(X[num_cols]).astype(np.float32)\nX_test[num_cols] = scaler.transform(X_test[num_cols]).astype(np.float32)\nprint_memory_usage(X, X_test, wording='after scale')\nclean_memory()\n\nnum_cols += new_enc_cols\nprint(\"prep len(cat_cols):\", len(cat_cols))\nprint(\"prep len(num_cols):\", len(num_cols), '\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:07:01.049076Z","iopub.execute_input":"2025-04-29T10:07:01.049512Z","iopub.status.idle":"2025-04-29T10:11:59.190408Z","shell.execute_reply.started":"2025-04-29T10:07:01.049467Z","shell.execute_reply":"2025-04-29T10:11:59.189091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790/notebook\nLR_START = 1e-7\nLR_MAX = 1e-3\nLR_MIN = 1e-7\nLR_RAMPUP_EPOCHS = 2\nLR_SUSTAIN_EPOCHS = 3\nEPOCHS = 9\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN    \n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nplt.xlabel('Epoch'); plt.ylabel('LR')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n      format(lr_y[0], max(lr_y), lr_y[-1]))\nLR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:11:59.192052Z","iopub.execute_input":"2025-04-29T10:11:59.192394Z","iopub.status.idle":"2025-04-29T10:11:59.435087Z","shell.execute_reply.started":"2025-04-29T10:11:59.192364Z","shell.execute_reply":"2025-04-29T10:11:59.433821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    TRAIN = True\n    FIT_VERBOSE = 2\n    folds = 5\n    epochs = 9\n    batch_size = 128\n    LR_Scheduler = [LR_Scheduler]\n    optimizer = Adam(learning_rate=1e-3)\n\n    conf = ModelConfig(auto_imputation=False,\n                       auto_discrete=True,\n                       fixed_embedding_dim=True,\n                       embeddings_output_dim=4,\n                       embedding_dropout=0.3,\n                       nets=['dnn_nets'],\n                       dnn_params={\n                           'hidden_units': ((1024, 0.3, True),\n                                             (512, 0.3, True),\n                                             (256, 0.3, True)),\n                           'dnn_activation': 'relu',\n                       },\n                       autoint_params={\n                            'num_attention': 3,\n                            'num_heads': 1,\n                            'dropout_rate': 0.0,\n                            'use_residual': True,\n                       },\n                       stacking_op='concat',\n                       output_use_bias=False,\n                       optimizer=optimizer,\n                       task='regression',\n                       loss='auto',\n                       metrics=['RootMeanSquaredError'],\n                       earlystopping_patience=1,\n                       )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:11:59.438225Z","iopub.execute_input":"2025-04-29T10:11:59.438552Z","iopub.status.idle":"2025-04-29T10:11:59.446534Z","shell.execute_reply.started":"2025-04-29T10:11:59.438524Z","shell.execute_reply":"2025-04-29T10:11:59.445054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(f\"/tmp/workdir/kaggle/input/predict-listening-deep-nn/models/\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:11:59.448224Z","iopub.execute_input":"2025-04-29T10:11:59.448590Z","iopub.status.idle":"2025-04-29T10:11:59.469373Z","shell.execute_reply.started":"2025-04-29T10:11:59.448556Z","shell.execute_reply":"2025-04-29T10:11:59.468098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.system(f\"cp -r /kaggle/input/predict-listening-deep-nn/models/* /tmp/workdir/kaggle/input/predict-listening-deep-nn/models/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:11:59.470715Z","iopub.execute_input":"2025-04-29T10:11:59.471056Z","iopub.status.idle":"2025-04-29T10:12:01.212499Z","shell.execute_reply.started":"2025-04-29T10:11:59.471025Z","shell.execute_reply":"2025-04-29T10:12:01.211301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir('/tmp/workdir/kaggle/input/predict-listening-deep-nn/models/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:12:01.214480Z","iopub.execute_input":"2025-04-29T10:12:01.215023Z","iopub.status.idle":"2025-04-29T10:12:01.222787Z","shell.execute_reply.started":"2025-04-29T10:12:01.214963Z","shell.execute_reply":"2025-04-29T10:12:01.221636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load models\ndef load_model(paths):\n    models = []\n    for fold in sorted(os.listdir(paths)):\n        path = os.path.join(paths, fold)\n        for file in os.listdir(path):\n            if file.endswith('.h5'):\n                models.append(DeepTable.load(path, file))\n    return models\n\nmodels = load_model(\"/kaggle/input/predict-listening-deep-nn/models\")\nprint(\"\\nmodels:\", models)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-29T10:12:01.224346Z","iopub.execute_input":"2025-04-29T10:12:01.224773Z","iopub.status.idle":"2025-04-29T10:12:28.019557Z","shell.execute_reply.started":"2025-04-29T10:12:01.224638Z","shell.execute_reply":"2025-04-29T10:12:28.018331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inference\nclass AvgModel:\n    def __init__(self, models: list[BaseEstimator]):\n        self.models = models\n    def predict(self, X: ArrayLike):\n        preds = []\n        for model in self.models:\n            pred = model.predict(X, verbose=1, batch_size=512).flatten()\n            preds.append(pred)\n        return np.mean(preds, axis=0)\n\navg_model = AvgModel(models)\ntest_pred = avg_model.predict(X_test)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-29T10:12:28.022121Z","iopub.execute_input":"2025-04-29T10:12:28.022491Z","iopub.status.idle":"2025-04-29T10:14:06.124958Z","shell.execute_reply.started":"2025-04-29T10:12:28.022458Z","shell.execute_reply":"2025-04-29T10:14:06.123367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")\nsub.Listening_Time_minutes = test_pred\nsub.to_csv(\"submission_1.csv\", index=False)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T10:14:06.126077Z","iopub.execute_input":"2025-04-29T10:14:06.126540Z","iopub.status.idle":"2025-04-29T10:14:06.574933Z","shell.execute_reply.started":"2025-04-29T10:14:06.126505Z","shell.execute_reply":"2025-04-29T10:14:06.573873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%reset -f","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CATBoost","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport pandas as pd\npd.options.mode.copy_on_write = True\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom cuml.preprocessing import TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.auto import tqdm\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import mean_squared_error\n\n# from lightgbm import LGBMRegressor\nimport lightgbm as lgb\n\ndef process_combinations_fast(df, columns_to_encode, pair_size, max_batch_size=2000):\n    # Precompute string versions of all columns once\n    str_df = df[columns_to_encode]\n    le = LabelEncoder()\n    str_df = str_df.astype(str)\n    total_new_cols = 0\n    \n    for r in pair_size:\n        print(f\"Processing {r}-combinations\")\n        \n        # Count total combinations for this r-value\n        n_combinations = np.math.comb(len(columns_to_encode), r)\n        print(f\"Total {r}-combinations to process: {n_combinations}\")\n        \n        # Process combinations in batches to manage memory\n        combos_iter = combinations(columns_to_encode, r)\n        batch_cols = []\n        batch_names = []\n        \n        with tqdm(total=n_combinations) as pbar:\n            while True:\n                # Collect a batch of combinations\n                batch_cols.clear()\n                batch_names.clear()\n                \n                # Fill the current batch\n                for _ in range(max_batch_size):\n                    try:\n                        cols = next(combos_iter)\n                        batch_cols.append(list(cols))\n                        batch_names.append('+'.join(cols))\n                    except StopIteration:\n                        break\n                \n                if not batch_cols:  # No more combinations\n                    break\n                \n                # Process this batch vectorized\n                for i, (cols, new_name) in enumerate(zip(batch_cols, batch_names)):\n                    # Fast vectorized concatenation\n                    result = str_df[cols[0]].copy()\n                    for col in cols[1:]:\n                        result += '' + str_df[col]\n                    \n                    df[new_name] = le.fit_transform(result) + 1\n                    pbar.update(1)\n                \n                total_new_cols += len(batch_cols)\n                if len(batch_cols) == max_batch_size:  # Only print on full batches\n                    print(f\"Progress: {total_new_cols}/{n_combinations} combinations processed\")\n        \n        print(f\"Completed all {r}-combinations. Total columns now: {len(df.columns)}\")\n    \n    return df\n\nTARGET = 'Listening_Time_minutes'\n# Load data\ndf_train = pd.read_csv(\"/kaggle/input/playground-series-s5e4/train.csv\")\ndf_train.drop(columns=['id'], inplace=True)\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\ndf_test.drop(columns=['id'], inplace=True)\n\noriginal = pd.read_csv('/kaggle/input/podcast-listening-time-prediction-dataset/podcast_dataset.csv')\n\noriginal_clean = original.dropna(subset=[TARGET]).drop_duplicates()\ndf_train = pd.concat([df_train, original_clean], axis=0, ignore_index=True)\n\ndf = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n\n# df.drop(columns=['id'], inplace=True)\ndf = df.drop_duplicates()\n\n# outlier removal\ndf['Episode_Length_minutes'] = np.maximum(0, np.minimum(120, df['Episode_Length_minutes']))\ndf['Host_Popularity_percentage'] = np.maximum(20, np.minimum(100, df['Host_Popularity_percentage']))\ndf['Guest_Popularity_percentage'] = np.maximum(0, np.minimum(100, df['Guest_Popularity_percentage']))\ndf['Host_Popularity_bin'] = pd.cut(df['Host_Popularity_percentage'], bins=[20,40,60,80,100], labels=[1,2,3,4])\ndf.loc[df['Number_of_Ads'] > 3, 'Number_of_Ads'] = 0\n\n# Encode categorical features\nday_mapping = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}\ndf['Publication_Day'] = df['Publication_Day'].map(day_mapping)\n\ntime_mapping = {'Morning': 1, 'Afternoon': 2, 'Evening': 3, 'Night': 4}\ndf['Publication_Time'] = df['Publication_Time'].map(time_mapping)\n\nsentiment_map = {'Negative': 1, 'Neutral': 2, 'Positive': 3}\ndf['Episode_Sentiment'] = df['Episode_Sentiment'].map(sentiment_map)\n\ndf['Episode_Title'] = df['Episode_Title'].str.replace('Episode ', '', regex=True)\ndf['Episode_Title'] = df['Episode_Title'].astype('int')\ndf['Title_Episode_Length'] = df['Episode_Title'] / (df['Episode_Length_minutes'] + 1)\nle = LabelEncoder()\nfor col in df.select_dtypes('object').columns:\n    df[col] = le.fit_transform(df[col]) + 1\n\n# Some Feature engineering\nfor col in ['Episode_Length_minutes']:\n    df[[col + '_sqrt', col + '_squared']] = np.column_stack([\n    np.sqrt(df[col]),\n    df[col] ** 2\n    ])\n\nfor col in tqdm(['Episode_Sentiment', 'Genre', 'Publication_Day', 'Podcast_Name', 'Episode_Title',\n                 'Guest_Popularity_percentage', 'Host_Popularity_percentage', 'Number_of_Ads']):\n    df[col + '_EP'] = df.groupby(col)['Episode_Length_minutes'].transform('mean')\n\ndf = process_combinations_fast(df, ['Episode_Length_minutes', 'Episode_Title', 'Publication_Time', 'Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', \n                     'Publication_Day', 'Podcast_Name','Genre','Guest_Popularity_percentage'], [2, 3, 5, 7], 1000) # [2, 3, 5, 7]\n\ndf = df.astype('float32')\n\ndf_train = df.iloc[:-len(df_test)]\ndf_test = df.iloc[-len(df_test):].reset_index(drop=True)\n\ndf_train = df_train[df_train['Listening_Time_minutes'].notnull()]\n\ntarget = df_train.pop('Listening_Time_minutes')\ndf_test.pop('Listening_Time_minutes')\n\ndf_train.shape, df_test.shape\n\n## Save and load model by pickle\n\"\"\"\nimport pickle\n\n# Suppose `model` is your trained LightGBM (or scikit-learn) estimator\n\n# 1) Save to file\nwith open('model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n# 2) Later—load from file\nwith open('model.pkl', 'rb') as f:\n    loaded_model = pickle.load(f)\n\n# 3) Use it exactly like the original\ny_pred = loaded_model.predict(X_new)\n\n\"\"\"\n\nimport pickle\nimport gc \n\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostRegressor, Pool\nfrom tqdm import tqdm\nimport pickle\n\n# seed and CV\nseed = 42\ncv = KFold(n_splits=7, random_state=seed, shuffle=True)\npred_test = np.zeros(df_test.shape[0])\n\n# CatBoost params\ncatboost_params = {\n    'iterations': 1_000_00,  # Large number, use early stopping\n    'learning_rate': 0.02,    # You can experiment with decaying manually\n    'depth': 8,               # CatBoost max_depth is 'depth'\n    'loss_function': 'RMSE',\n    'eval_metric': 'RMSE',\n    'random_seed': seed,\n    'verbose': 500,\n    'early_stopping_rounds': 30,\n    'task_type': 'GPU',       # Use 'CPU' if GPU not available\n    'od_type': 'Iter',\n}\n\nall_histories = []\nfeatures = df_train.columns.tolist()\n\nfor fold, (trn_idx, val_idx) in enumerate(cv.split(df_train), 1):\n    print(f\"Starting Fold {fold}\")\n    X_trn, y_trn = df_train.iloc[trn_idx].copy(), target.iloc[trn_idx]\n    X_val, y_val = df_train.iloc[val_idx].copy(), target.iloc[val_idx]\n    X_sub = df_test[X_trn.columns.tolist()].copy()\n\n    # === Target Encoding ===\n    encoder = TargetEncoder(n_folds=5, seed=seed, stat=\"mean\")\n    print(f\"Fold {fold}: Applying Target Encoding...\")\n\n    # first 20 new cols\n    for col in tqdm(features[:20], desc=f\"Fold {fold} TE‐add\"):\n        # Ensure the column exists in the dataframes before encoding\n        if col in X_trn.columns:\n            X_trn[f\"{col}_te\"] = encoder.fit_transform(X_trn[[col]], y_trn)\n            X_val[f\"{col}_te\"] = encoder.transform(X_val[[col]])\n            X_sub[f\"{col}_te\"] = encoder.transform(X_sub[[col]])\n        else:\n            print(f\"Warning: Column '{col}' not found in training data for TE-add.\")\n\n    # remaining, in‐place\n    for col in tqdm(features[20:], desc=f\"Fold {fold} TE‐replace\"):\n         # Ensure the column exists in the dataframes before encoding\n        if col in X_trn.columns:\n            X_trn[col] = encoder.fit_transform(X_trn[[col]], y_trn)\n            X_val[col] = encoder.transform(X_val[[col]])\n            X_sub[col] = encoder.transform(X_sub[[col]])\n        else:\n             print(f\"Warning: Column '{col}' not found in training data for TE-replace.\")\n\n    # Create CatBoost pools\n    train_pool = Pool(X_trn, label=y_trn)\n    valid_pool = Pool(X_val, label=y_val)\n\n    print(f\"Fold {fold}: Training CatBoost model...\")\n    model = CatBoostRegressor(**catboost_params)\n    model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n    \n\n    # Predict on test\n    test_pred = model.predict(X_sub)\n    pred_test += np.clip(test_pred, 0, 120)\n\n    print(f\"Fold {fold} finished, best_iteration={model.get_best_iteration()}\")\n    print(\"-\" * 60)\n\n    del model\n    del encoder\n    del X_trn, y_trn, X_val, y_val, X_sub\n    gc.collect()\n\n# average over folds\npred_test /= cv.n_splits\n\nprint(\"Training complete. Test predictions averaged across folds.\")\n\n\npred_test\n\ndf_sub = pd.read_csv(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")\ndf_sub.Listening_Time_minutes = pred_test\ndf_sub.to_csv('submission_2.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%reset -f","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.simplefilter('ignore')\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass OrderedTargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Out‑of‑fold **mean‑rank** encoder with optional smoothing.\n    • Encodes each category by the *rank* of its target mean within a fold.\n    • Unseen categories get the global mean rank (or −1 if you prefer).\n    \"\"\"\n    def __init__(self, cat_cols=None, n_splits=5, smoothing=0):\n        self.cat_cols   = cat_cols\n        self.n_splits   = n_splits\n        self.smoothing  = smoothing       # 0 = no smoothing\n        self.maps_      = {}              # per‑fold maps\n        self.global_map = {}              # fit on full data for test set\n\n    def _make_fold_map(self, X_col, y):\n        means = y.groupby(X_col, dropna=False).mean()\n        if self.smoothing > 0:\n            counts = y.groupby(X_col, dropna=False).count()\n            smooth = (counts * means + self.smoothing * y.mean()) / (counts + self.smoothing)\n            means  = smooth\n        return {k: r for r, k in enumerate(means.sort_values().index)}\n\n    def fit(self, X, y):\n        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n        if self.cat_cols is None:\n            self.cat_cols = X.select_dtypes(include='object').columns.tolist()\n\n        kf = KFold(self.n_splits, shuffle=True, random_state=42)\n        self.maps_ = {col: [None]*self.n_splits for col in self.cat_cols}\n\n        for fold, (tr_idx, _) in enumerate(kf.split(X)):\n            X_tr, y_tr = X.loc[tr_idx], y.loc[tr_idx]\n            for col in self.cat_cols:\n                self.maps_[col][fold] = self._make_fold_map(X_tr[col], y_tr)\n\n        for col in self.cat_cols:\n            self.global_map[col] = self._make_fold_map(X[col], y)\n\n        return self\n\n    def transform(self, X, y=None, fold=None):\n        \"\"\"\n        • During CV pass fold index to use fold‑specific maps (leak‑free).\n        • At inference time (fold=None) uses global map.\n        \"\"\"\n        X = X.copy()\n        tgt_maps = {col: (self.global_map[col] if fold is None else self.maps_[col][fold])\n                    for col in self.cat_cols}\n        for col, mapping in tgt_maps.items():\n            X[col] = X[col].map(mapping).fillna(-1).astype(int)\n        return X\n\ndef target_encode(df_train, df_val, col, target, stats='mean', prefix='TE'):\n    df_val = df_val.copy()\n    agg = df_train.groupby(col)[target].agg(stats)    \n    if isinstance(stats, (list, tuple)):\n        for s in stats:\n            colname = f\"{prefix}_{col}_{s}\"\n            df_val[colname] = df_val[col].map(agg[s]).astype(float)\n            # df_val[colname].fillna(agg[s].mean(), inplace=True)\n    else:\n        suffix = stats if isinstance(stats, str) else stats.__name__\n        colname = f\"{prefix}_{col}_{suffix}\"\n        df_val[colname] = df_val[col].map(agg).astype(float)\n        df_val[colname].fillna(agg.mean(), inplace=True)\n    return df_val\n\nencoded_columns = []\n\nselected_comb = [\n     ['Episode_Length_minutes', 'Host_Popularity_percentage'],\n    ['Episode_Length_minutes', 'Guest_Popularity_percentage'],\n    ['Episode_Length_minutes', 'Number_of_Ads'],\n    ['Episode_Num', 'Host_Popularity_percentage'],\n    ['Episode_Num', 'Guest_Popularity_percentage'],\n    ['Episode_Num', 'Number_of_Ads'],    \n    ['Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n    ['Host_Popularity_percentage', 'Number_of_Ads'],\n    ['Host_Popularity_percentage', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Podcast_Name'],\n    ['Episode_Num', 'Podcast_Name'],  \n    ['Guest_Popularity_percentage', 'Podcast_Name'],\n    ['ELm_r1', 'Episode_Num'],\n    ['ELm_r1', 'Host_Popularity_percentage'], \n    ['ELm_r1', 'Guest_Popularity_percentage'],\n    ['ELm_r2', 'Episode_Num'],\n    ['ELm_r2', 'Episode_Sentiment'],\n    ['ELm_r2', 'Publication_Day'],\n    ['Linear_Feature', 'Number_of_Ads'],\n    ['Linear_Feature', 'Genre'],\n    ['Linear_Feature', 'Episode_Sentiment'],\n\n    \n    # 3-interaction\n    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Number_of_Ads', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Number_of_Ads', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Episode_Sentiment', 'Publication_Time'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Day'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Time'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Genre'],\n    ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n    ['Episode_Num', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n    ['Episode_Num', 'Guest_Popularity_percentage', 'Publication_Day'],\n    ['Episode_Num', 'Guest_Popularity_percentage', 'Publication_Time'],\n    ['Episode_Num', 'Guest_Popularity_percentage', 'Genre'],\n    ['Episode_Num', 'Number_of_Ads', 'Episode_Sentiment'],\n    ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n    ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n    ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Day'],\n    ['Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Time'],\n    ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n    ['Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n    ['Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],   \n    ['ELm_r1', 'Number_of_Ads', 'Episode_Sentiment'],\n    ['ELm_r2', 'Number_of_Ads', 'Podcast_Name'],\n    ['Linear_Feature', 'Podcast_Name', 'Episode_Num'], # add  for test\n    ['Linear_Feature', 'Genre', 'Number_of_Ads'], # add for test\n    \n    # 4-interaction\n    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Host_Popularity_percentage', 'Genre'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Guest_Popularity_percentage', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Number_of_Ads', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Publication_Day', 'Genre'],    \n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Host_Popularity_percentage', 'Publication_Day', 'Genre'],\n    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Publication_Time'],\n    ['Episode_Length_minutes', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],\n    ['Episode_Length_minutes', 'Episode_Num', 'Publication_Time', 'Podcast_Name'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Episode_Sentiment'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Time'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Publication_Day'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Publication_Time'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Genre'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Publication_Time', 'Genre'],\n    ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment'],\n    ['Episode_Num', 'Guest_Popularity_percentage', 'Number_of_Ads', 'Genre'],\n    ['Episode_Num', 'Host_Popularity_percentage', 'Episode_Sentiment', 'Podcast_Name'],\n    ['Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', 'Podcast_Name'],\n    ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Day', 'Podcast_Name'],\n    ['Host_Popularity_percentage', 'Number_of_Ads', 'Publication_Time', 'Podcast_Name'],\n    \n]\n\ndef feature_engineer(data1, data2):\n    data1['is_train'] = 1\n    data2['is_train'] = 0\n    combined_dataset = pd.concat([data1, data2], ignore_index = True)\n    combined_dataset['Episode_Num'] = combined_dataset['Episode_Title'].str[8:]\n    combined_dataset['is_weekend'] = combined_dataset['Publication_Day'].isin(['Saturday', 'Sunday']).astype(int)\n    combined_dataset = combined_dataset.drop(columns = ['Episode_Title'])\n    # fill nan value\n    combined_dataset['Episode_Length_minutes'] = combined_dataset['Episode_Length_minutes'].fillna(combined_dataset['Episode_Length_minutes'].median())\n    combined_dataset['Guest_Popularity_percentage'] = combined_dataset['Guest_Popularity_percentage'].fillna(combined_dataset['Guest_Popularity_percentage'].median())\n    combined_dataset['Number_of_Ads'] = combined_dataset['Number_of_Ads'].fillna(combined_dataset['Number_of_Ads'].median())\n    # add linear feature\n    combined_dataset['Linear_Feature'] = 0.72 * combined_dataset['Episode_Length_minutes']\n    ELM = []\n    for k in range(3):\n        col_name = f'ELm_r{k}'\n        combined_dataset[col_name] = combined_dataset['Episode_Length_minutes'].round(k)\n        ELM.append(col_name)\n    for comb in selected_comb:\n        name = '_'.join(comb)\n            \n        if len(comb) == 2:\n            combined_dataset[name] = combined_dataset[comb[0]].astype(str) + '_' + combined_dataset[comb[1]].astype(str)\n        elif len(comb) == 3:\n            combined_dataset[name] = (combined_dataset[comb[0]].astype(str) + '_' +\n                                   combined_dataset[comb[1]].astype(str) + '_' +\n                                   combined_dataset[comb[2]].astype(str))\n        elif len(comb) == 4:\n            combined_dataset[name] = (combined_dataset[comb[0]].astype(str) + '_' +\n                                   combined_dataset[comb[1]].astype(str) + '_' +\n                                   combined_dataset[comb[2]].astype(str) + '_' +\n                                   combined_dataset[comb[3]].astype(str))\n    \n        encoded_columns.append(name)\n    combined_dataset[encoded_columns] = combined_dataset[encoded_columns].astype('category')\n    # divide back to train and test dataset\n    train_dataset = combined_dataset[combined_dataset['is_train'] == 1]\n    test_dataset = combined_dataset[combined_dataset['is_train'] == 0]\n    train_dataset = train_dataset.drop(columns=['is_train'])\n    test_dataset = test_dataset.drop(columns = ['is_train'])\n    return train_dataset, test_dataset\n\nTARGET = 'Listening_Time_minutes'\nCATS = ['Podcast_Name', 'Episode_Num', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\nNUMS = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n        'Guest_Popularity_percentage', 'Number_of_Ads', 'Linear_Feature']\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport pandas as pd\nencode_stats = ['mean']\nFOLDS = 7 # Both 2 model got 7 folds\nimport gc\nimport pickle\ndef xgBoost118Predict():\n    train_dataset = pd.read_csv('/kaggle/input/playground-series-s5e4/train.csv')\n    test_dataset = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\n    original_dataset = pd.read_csv('/kaggle/input/podcast-listening-time-prediction-dataset/podcast_dataset.csv')\n    original_dataset = original_dataset.dropna(subset=[TARGET]).drop_duplicates()\n    \n    # for col in CATS:\n    #     train_dataset[col] = train_dataset[col].astype('category')\n    #     test_dataset[col]  = test_dataset[col].astype('category')\n\n    train_dataset = pd.concat([train_dataset, original_dataset], axis=0, ignore_index=True)\n    print('Starting Engineering')\n    train_dataset, test_dataset = feature_engineer(train_dataset, test_dataset)\n    print('Finish Engineering')\n    # y_train = train_dataset[TARGET]; train_dataset.drop(TARGET, axis=1, inplace=True)\n    # test_ids = test_dataset['id'].values\n    test_dataset.drop(TARGET, axis=1, inplace=True)\n\n    FEATURES = NUMS + CATS + encoded_columns\n\n    \n    print('Start Predict model')\n    pred = np.zeros(len(test_dataset))\n    outer_kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n    for fold, (tr_idx, vl_idx) in enumerate(outer_kf.split(train_dataset), 1):\n        print(f\"-- Fold {fold}/{FOLDS} --\")\n        X_tr_raw = train_dataset.loc[tr_idx, FEATURES].reset_index(drop=True)\n        y_tr     = train_dataset.loc[tr_idx, TARGET].reset_index(drop=True)\n        X_vl_raw = train_dataset.loc[vl_idx, FEATURES].reset_index(drop=True)\n        y_vl     = train_dataset.loc[vl_idx, TARGET].reset_index(drop=True)\n        X_ts_raw = test_dataset[FEATURES].copy()\n\n        # 5) Make fresh copies for encoding\n        X_tr = X_tr_raw.copy()\n        X_vl = X_vl_raw.copy()\n        X_ts = X_ts_raw.copy()\n\n        # 6) Inner CV for target‐encode\n        inner_kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n        for in_tr_idx, in_vl_idx in inner_kf.split(X_tr_raw):\n            # build small training fold (features + target)\n            in_tr = pd.concat([\n                X_tr_raw.loc[in_tr_idx].reset_index(drop=True),\n                y_tr.loc[in_tr_idx].reset_index(drop=True)\n            ], axis=1)\n            in_vl = X_tr_raw.loc[in_vl_idx].reset_index(drop=True)\n        \n            for col in encoded_columns:\n                for stat in encode_stats:\n                    te_tmp = target_encode(\n                        in_tr, in_vl.copy(),\n                        col, TARGET,\n                        stats=stat,\n                        prefix='TE'\n                    )\n                    te_col = f\"TE_{col}_{stat}\"\n                    X_tr.loc[in_vl_idx, te_col] = te_tmp[te_col].values\n        \n            # free inner-fold intermediates\n            del in_tr, in_vl, te_tmp\n        gc.collect()\n\n        # 7) Encode valid & test just once\n        full_tr = pd.concat([X_tr_raw, y_tr], axis=1)\n        for col in encoded_columns:\n            for stat in encode_stats:\n                X_vl = target_encode(\n                    full_tr, X_vl, col, TARGET,\n                    stats=stat,\n                    prefix='TE'\n                )\n                X_ts = target_encode(\n                    full_tr, X_ts, col, TARGET,\n                    stats=stat,\n                    prefix='TE'\n                )\n        gc.collect()\n\n        X_tr.drop(columns=encoded_columns, inplace=True)\n        X_vl.drop(columns=encoded_columns, inplace=True)\n        X_ts.drop(columns=encoded_columns, inplace=True)\n        \n        enc = OrderedTargetEncoder(cat_cols=CATS, n_splits=FOLDS, smoothing=20)\n        enc.fit(X_tr, y_tr)\n        X_tr[CATS] = enc.transform(X_tr[CATS])\n        X_vl[CATS] = enc.transform(X_vl[CATS])\n        X_ts[CATS] = enc.transform(X_ts[CATS])\n\n        with open(f'/kaggle/input/xgboostver11-8/xgb_model{fold}.pkl','rb') as f:\n            model = pickle.load(f)\n        pred += model.predict(X_ts)\n\n        del X_tr_raw, X_vl_raw\n        del X_tr, X_vl, X_ts\n        del y_tr, y_vl, enc, model\n        gc.collect()\n        print(f'Finish fold {fold}')\n    print('Finish predicting task')\n    del train_dataset, test_dataset, original_dataset\n    gc.collect()\n    return pred / FOLDS\n\ndef runThis():\n    predict_from_model_1 = xgBoost118Predict()\n    print(predict_from_model_1)\n    test_dataset = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\n    test_id = test_dataset['id'].unique()\n    # predict_from_model_2 = xgboost117predict()\n    # print(predict_from_model_2)\n    pred_final = predict_from_model_1\n    print(pred_final)\n    df_sub = pd.read_csv(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")\n    df_sub.Listening_Time_minutes = pred_final\n    df_sub.to_csv('submission_3.csv', index = False)\nrunThis()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%reset -f","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport pandas as pd\npd.options.mode.copy_on_write = True\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom cuml.preprocessing import TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.auto import tqdm\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import mean_squared_error\n\n# from lightgbm import LGBMRegressor\nimport lightgbm as lgb\n\ndef process_combinations_fast(df, columns_to_encode, pair_size, max_batch_size=2000):\n    # Precompute string versions of all columns once\n    str_df = df[columns_to_encode]\n    le = LabelEncoder()\n    str_df = str_df.astype(str)\n    total_new_cols = 0\n    \n    for r in pair_size:\n        print(f\"Processing {r}-combinations\")\n        \n        # Count total combinations for this r-value\n        n_combinations = np.math.comb(len(columns_to_encode), r)\n        print(f\"Total {r}-combinations to process: {n_combinations}\")\n        \n        # Process combinations in batches to manage memory\n        combos_iter = combinations(columns_to_encode, r)\n        batch_cols = []\n        batch_names = []\n        \n        with tqdm(total=n_combinations) as pbar:\n            while True:\n                # Collect a batch of combinations\n                batch_cols.clear()\n                batch_names.clear()\n                \n                # Fill the current batch\n                for _ in range(max_batch_size):\n                    try:\n                        cols = next(combos_iter)\n                        batch_cols.append(list(cols))\n                        batch_names.append('+'.join(cols))\n                    except StopIteration:\n                        break\n                \n                if not batch_cols:  # No more combinations\n                    break\n                \n                # Process this batch vectorized\n                for i, (cols, new_name) in enumerate(zip(batch_cols, batch_names)):\n                    # Fast vectorized concatenation\n                    result = str_df[cols[0]].copy()\n                    for col in cols[1:]:\n                        result += '' + str_df[col]\n                    \n                    df[new_name] = le.fit_transform(result) + 1\n                    pbar.update(1)\n                \n                total_new_cols += len(batch_cols)\n                if len(batch_cols) == max_batch_size:  # Only print on full batches\n                    print(f\"Progress: {total_new_cols}/{n_combinations} combinations processed\")\n        \n        print(f\"Completed all {r}-combinations. Total columns now: {len(df.columns)}\")\n    \n    return df\n\nTARGET = 'Listening_Time_minutes'\n# Load data\ndf_train = pd.read_csv(\"/kaggle/input/playground-series-s5e4/train.csv\")\ndf_train.drop(columns=['id'], inplace=True)\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\ndf_test.drop(columns=['id'], inplace=True)\n\noriginal = pd.read_csv('/kaggle/input/podcast-listening-time-prediction-dataset/podcast_dataset.csv')\n\noriginal_clean = original.dropna(subset=[TARGET]).drop_duplicates()\ndf_train = pd.concat([df_train, original_clean], axis=0, ignore_index=True)\n\ndf = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n\n# df.drop(columns=['id'], inplace=True)\ndf = df.drop_duplicates()\n\n# outlier removal\ndf['Episode_Length_minutes'] = np.maximum(0, np.minimum(120, df['Episode_Length_minutes']))\ndf['Host_Popularity_percentage'] = np.maximum(20, np.minimum(100, df['Host_Popularity_percentage']))\ndf['Guest_Popularity_percentage'] = np.maximum(0, np.minimum(100, df['Guest_Popularity_percentage']))\ndf['Host_Popularity_bin'] = pd.cut(df['Host_Popularity_percentage'], bins=[20,40,60,80,100], labels=[1,2,3,4])\ndf.loc[df['Number_of_Ads'] > 3, 'Number_of_Ads'] = 0\n\n# Encode categorical features\nday_mapping = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}\ndf['Publication_Day'] = df['Publication_Day'].map(day_mapping)\n\ntime_mapping = {'Morning': 1, 'Afternoon': 2, 'Evening': 3, 'Night': 4}\ndf['Publication_Time'] = df['Publication_Time'].map(time_mapping)\n\nsentiment_map = {'Negative': 1, 'Neutral': 2, 'Positive': 3}\ndf['Episode_Sentiment'] = df['Episode_Sentiment'].map(sentiment_map)\n\ndf['Episode_Title'] = df['Episode_Title'].str.replace('Episode ', '', regex=True)\ndf['Episode_Title'] = df['Episode_Title'].astype('int')\ndf['Title_Episode_Length'] = df['Episode_Title'] / (df['Episode_Length_minutes'] + 1)\nle = LabelEncoder()\nfor col in df.select_dtypes('object').columns:\n    df[col] = le.fit_transform(df[col]) + 1\n\n# Some Feature engineering\nfor col in ['Episode_Length_minutes']:\n    df[[col + '_sqrt', col + '_squared']] = np.column_stack([\n    np.sqrt(df[col]),\n    df[col] ** 2\n    ])\n\nfor col in tqdm(['Episode_Sentiment', 'Genre', 'Publication_Day', 'Podcast_Name', 'Episode_Title',\n                 'Guest_Popularity_percentage', 'Host_Popularity_percentage', 'Number_of_Ads']):\n    df[col + '_EP'] = df.groupby(col)['Episode_Length_minutes'].transform('mean')\n\ndf = process_combinations_fast(df, ['Episode_Length_minutes', 'Episode_Title', 'Publication_Time', 'Host_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment', \n                     'Publication_Day', 'Podcast_Name','Genre','Guest_Popularity_percentage'], [2, 3, 5, 7], 1000) # [2, 3, 5, 7]\n\ndf = df.astype('float32')\n\ndf_train = df.iloc[:-len(df_test)]\ndf_test = df.iloc[-len(df_test):].reset_index(drop=True)\n\ndf_train = df_train[df_train['Listening_Time_minutes'].notnull()]\n\ntarget = df_train.pop('Listening_Time_minutes')\ndf_test.pop('Listening_Time_minutes')\n\ndf_train.shape, df_test.shape\n\nimport pickle\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom tqdm import tqdm\n# Assuming TargetEncoder is imported from somewhere else, e.g., category_encoders\n# from category_encoders import TargetEncoder\n\nseed = 42\ncv = KFold(n_splits=7, random_state=seed, shuffle=True)\npred_test = np.zeros(df_test.shape[0])\n\n# exponentially decaying LR schedule\n# Corrected: Accept CallbackEnv object and extract iteration\ndef lr_decay(env):\n    \"\"\"\n    Exponentially decaying learning rate schedule.\n    Args:\n        env (lgb.CallbackEnv): The callback environment object.\n    Returns:\n        float: The learning rate for the current round.\n    \"\"\"\n    current_round = env.iteration # Extract current iteration from CallbackEnv\n    lr_start, lr_end, decay_speed = 0.02, 0.005, 0.01\n    return lr_end + (lr_start - lr_end) * np.exp(-decay_speed * current_round)\n\n# callbacks\n# Pass the lr_decay function directly, not wrapped in LearningRateScheduler\n# The evals_result dictionary will be populated by the early_stopping callback\nevals_result = {} # Define evals_result dictionary here\n# early_stopping callback handles verbosity\nearly_stop_callback = lgb.callback.early_stopping(stopping_rounds=30, first_metric_only=True, verbose=500) # Set verbose here\n\n# LightGBM params\nlgbm_params = {\n    'objective':        'regression_l2',\n    'metric':           'rmse',\n    'seed':             seed,\n    'max_depth':        -1,\n    # The initial learning rate set here will be overridden by the scheduler\n    'learning_rate':    0.04,\n    'num_leaves':       512,\n    'colsample_bytree': 0.2,\n    # Corrected: Reduced max_bin for GPU compatibility\n    'max_bin':          255, # Changed from 512 to 255 for GPU\n    'verbosity':        -1, # This controls general LightGBM output, not evaluation print frequency\n    'device':           'gpu'  # use 'cpu' if you don’t have GPU support\n}\n\nall_histories = []\n# Assuming 'features' is correctly derived from df_train columns before the loop\nfeatures = df_train.columns.tolist()\n\nfor fold, (trn_idx, val_idx) in enumerate(cv.split(df_train), 1):\n    print(f\"Starting Fold {fold}\")\n    X_trn, y_trn = df_train.iloc[trn_idx].copy(), target.iloc[trn_idx]\n    X_val, y_val = df_train.iloc[val_idx].copy(), target.iloc[val_idx]\n    # Ensure X_sub has the same columns as X_trn before adding TE features\n    X_sub  = df_test[X_trn.columns.tolist()].copy()\n\n\n    # === Target‐encoding ===\n    # Assuming TargetEncoder is defined and imported correctly\n    encoder = TargetEncoder(n_folds=5, seed=seed, stat=\"mean\")\n    print(f\"Fold {fold}: Applying Target Encoding...\")\n\n    # first 20 new cols\n    for col in tqdm(features[:20], desc=f\"Fold {fold} TE‐add\"):\n        # Ensure the column exists in the dataframes before encoding\n        if col in X_trn.columns:\n            X_trn[f\"{col}_te\"] = encoder.fit_transform(X_trn[[col]], y_trn)\n            X_val[f\"{col}_te\"] = encoder.transform(X_val[[col]])\n            X_sub[f\"{col}_te\"] = encoder.transform(X_sub[[col]])\n        else:\n            print(f\"Warning: Column '{col}' not found in training data for TE-add.\")\n\n    # remaining, in‐place\n    for col in tqdm(features[20:], desc=f\"Fold {fold} TE‐replace\"):\n         # Ensure the column exists in the dataframes before encoding\n        if col in X_trn.columns:\n            X_trn[col] = encoder.fit_transform(X_trn[[col]], y_trn)\n            X_val[col] = encoder.transform(X_val[[col]])\n            X_sub[col] = encoder.transform(X_sub[[col]])\n        else:\n             print(f\"Warning: Column '{col}' not found in training data for TE-replace.\")\n\n\n    # === Create LightGBM datasets ===\n    dtrain = lgb.Dataset(X_trn, label=y_trn)\n    dvalid = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n\n    # evals_result dictionary is defined before the loop and populated by callbacks\n    # evals_result = {} # Removed: Defined outside the loop\n\n    print(f\"Fold {fold}: Training LightGBM model...\")\n    model = lgb.train(\n        params              = lgbm_params,\n        train_set           = dtrain,\n        num_boost_round     = 1_000_000, # Set a large number, early stopping will stop it\n        valid_sets          = [dtrain, dvalid],\n        valid_names         = ['train', 'valid'],\n        # early_stopping_rounds is now handled by the callback\n        # early_stopping_rounds = 30,\n        callbacks           = [lr_decay, early_stop_callback], # Pass the function directly\n        # Removed: evals_result is not a direct keyword argument for lgb.train\n        # evals_result        = evals_result,\n        # Removed: verbose_eval is handled by callbacks\n        # verbose_eval        = 500\n    )\n\n\n    # The evals_result dictionary defined before the loop will now contain the training history\n    all_histories.append(evals_result.copy()) # Append a copy of the results for this fold\n\n    # If you have a plotting utility that expects a history dict:\n    # plot_training_history(evals_result) # Assuming plot_training_history is defined\n\n    # Predict on validation (if you need it for evaluation metrics within the loop)\n    # val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n    # Predict on test fold and clip\n    test_pred = model.predict(X_sub, num_iteration=model.best_iteration)\n    pred_test += np.clip(test_pred, 0, 120)\n\n    print(f\"Fold {fold} finished, best_iteration={model.best_iteration}\")\n    print(\"-\" * 60)\n\n# average over folds\npred_test /= cv.n_splits\n\nprint(\"Training complete. Test predictions averaged across folds.\")\n\npred_test\n\ndf_sub = pd.read_csv(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")\ndf_sub.Listening_Time_minutes = pred_test\ndf_sub.to_csv('submission_4.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read file here\n\ndf1 = pd.read_csv('/kaggle/working/submission_1.csv')\ndf2 = pd.read_csv('/kaggle/working/submission_2.csv')\ndf3 = pd.read_csv('/kaggle/working/submission_3.csv')\ndf4 = pd.read_csv('/kaggle/working/submission_4.csv')\n\ndf = pd.read_csv(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")\ndf['Listening_Time_minutes'] = 0.23 * df1['Listening_Time_minutes'] + 0.15 * df2['Listening_Time_minutes'] + 0.32 * df3['Listening_Time_minutes'] + 0.3 * df4['Listening_Time_minutes']\n\ndf.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}